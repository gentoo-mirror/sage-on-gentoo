diff --git a/networkx/algorithms/link_analysis/hits_alg.py b/networkx/algorithms/link_analysis/hits_alg.py
index 1c95639..483b16c 100644
--- a/networkx/algorithms/link_analysis/hits_alg.py
+++ b/networkx/algorithms/link_analysis/hits_alg.py
@@ -294,7 +294,7 @@ def hits_scipy(G, max_iter=100, tol=1.0e-6, normalized=True):
         x = A * x
         x = x / x.max()
         # check convergence, l1 norm
-        err = scipy.absolute(x - xlast).sum()
+        err = np.absolute(x - xlast).sum()
         if err < tol:
             break
         if i > max_iter:
diff --git a/networkx/algorithms/link_analysis/pagerank_alg.py b/networkx/algorithms/link_analysis/pagerank_alg.py
index 8ad45b9..210bed7 100644
--- a/networkx/algorithms/link_analysis/pagerank_alg.py
+++ b/networkx/algorithms/link_analysis/pagerank_alg.py
@@ -421,6 +421,7 @@ def pagerank_scipy(G, alpha=0.85, personalization=None,
        http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&doc=1999-66&format=pdf
     """
     import scipy.sparse
+    import numpy
 
     N = len(G)
     if N == 0:
@@ -429,23 +430,23 @@ def pagerank_scipy(G, alpha=0.85, personalization=None,
     nodelist = list(G)
     M = nx.to_scipy_sparse_matrix(G, nodelist=nodelist, weight=weight,
                                   dtype=float)
-    S = scipy.array(M.sum(axis=1)).flatten()
+    S = numpy.array(M.sum(axis=1)).flatten()
     S[S != 0] = 1.0 / S[S != 0]
     Q = scipy.sparse.spdiags(S.T, 0, *M.shape, format='csr')
     M = Q * M
 
     # initial vector
     if nstart is None:
-        x = scipy.repeat(1.0 / N, N)
+        x = numpy.repeat(1.0 / N, N)
     else:
-        x = scipy.array([nstart.get(n, 0) for n in nodelist], dtype=float)
+        x = numpy.array([nstart.get(n, 0) for n in nodelist], dtype=float)
         x = x / x.sum()
 
     # Personalization vector
     if personalization is None:
-        p = scipy.repeat(1.0 / N, N)
+        p = numpy.repeat(1.0 / N, N)
     else:
-        p = scipy.array([personalization.get(n, 0) for n in nodelist], dtype=float)
+        p = numpy.array([personalization.get(n, 0) for n in nodelist], dtype=float)
         p = p / p.sum()
 
     # Dangling nodes
@@ -453,10 +454,10 @@ def pagerank_scipy(G, alpha=0.85, personalization=None,
         dangling_weights = p
     else:
         # Convert the dangling dictionary into an array in nodelist order
-        dangling_weights = scipy.array([dangling.get(n, 0) for n in nodelist],
+        dangling_weights = numpy.array([dangling.get(n, 0) for n in nodelist],
                                        dtype=float)
         dangling_weights /= dangling_weights.sum()
-    is_dangling = scipy.where(S == 0)[0]
+    is_dangling = numpy.where(S == 0)[0]
 
     # power iteration: make up to max_iter iterations
     for _ in range(max_iter):
@@ -464,7 +465,7 @@ def pagerank_scipy(G, alpha=0.85, personalization=None,
         x = alpha * (x * M + sum(x[is_dangling]) * dangling_weights) + \
             (1 - alpha) * p
         # check convergence, l1 norm
-        err = scipy.absolute(x - xlast).sum()
+        err = numpy.absolute(x - xlast).sum()
         if err < N * tol:
             return dict(zip(nodelist, map(float, x)))
     raise nx.PowerIterationFailedConvergence(max_iter)
